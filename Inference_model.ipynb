{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "import os\n",
    "from nltk import tokenize\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "rouge = Rouge()\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from summa.summarizer import summarize\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentences):\n",
    "\n",
    "    #Load Google pre-trained words \n",
    "    embedding_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        'C:/Users/amitp/Downloads/extractive-document-summarization-master/word2vec/GoogleNews-vectors-negative300.bin.gz', \n",
    "        binary=True, \n",
    "        limit=50000)\n",
    "    word_vectors = embedding_model.wv\n",
    "    max_sen_len = 250\n",
    "    #tokenize sentences\n",
    "    tokenizer = Tokenizer(num_words=30000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences) # replace words with its wordid\n",
    "    padded_sequences = pad_sequences(sequences,maxlen=max_sen_len)\n",
    "    word_index = tokenizer.word_index\n",
    "    # word embedding with 300 dimensions\n",
    "    embedding_weights = {key: embedding_model[word] if word in word_vectors.vocab else\n",
    "                                  np.random.uniform(-0.25, 0.25, word_vectors.vector_size)\n",
    "                            for word, key in word_index.items()}\n",
    "    embedding_weights[0] = np.zeros(word_vectors.vector_size)\n",
    "    #Build a 3D array: 1D fnumber of sentences, 1D for the no of words and 1D for word embedding. \n",
    "    embedded_sentences = np.stack([np.stack([embedding_weights[t] for t in s]) for s in padded_sequences])\n",
    "    return embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_testing(Id,text,sumry):\n",
    "    test_data = []\n",
    "    for i,j in enumerate(Id):\n",
    "        TES = np.ones((len(text[i]), 3), dtype=object) \n",
    "        TES[:,0] = \"dummy\"\n",
    "        TES[:,1] = text[i]\n",
    "        embedding = sentence_embedding(text[i])\n",
    "        #embedding = embedding[0::2]\n",
    "        test_data.append((np.array(text[i]), np.array(embedding), np.array(sumry[i])))\n",
    "        print(\"Finished\", i, \"of\", len(Id)+1,\"sentences --\", i/(len(Id)+1),\"%\", end='\\r')\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(test_data):\n",
    "    model_summary = []\n",
    "    for d,doc in enumerate(test_data): \n",
    "\n",
    "        sentences = doc[0]  # sentences in document\n",
    "        x_test_old = doc[1] # embedding of doc\n",
    "        s1 = x_test_old.shape[0] # number of sentences in document\n",
    "        (s3,s4) = x_test_old[0].shape # sentence length,embedding dimension = 300\n",
    "\n",
    "        # Data reshaping for fitting it to model.\n",
    "\n",
    "        x_test = np.random.rand(s1,250,s4,1) # random initialization of tenst tensor\n",
    "        try:\n",
    "            for i in range(s1) :\n",
    "                s = np.array( [ np.pad(x_test_old[i],((250-s3,0),(0,0)),mode='constant') ] )[0] # pad sentences to 250 len\n",
    "                s = np.expand_dims(s, 2)\n",
    "                x_test[i] = s\n",
    "\n",
    "            predicted_scores = model1.predict(x_test, batch_size=256)\n",
    "            argsorted_scores = np.argpartition(np.transpose(predicted_scores)[0], 1)\n",
    "\n",
    "            true_summary = doc[2]\n",
    "            predicted_summary = []\n",
    "            summary_length = 0\n",
    "\n",
    "            i = 0\n",
    "            while i < len(sentences) and summary_length < 250: \n",
    "                sentence = sentences[argsorted_scores[i]]\n",
    "                #if ( dummy_rouge( sentence , predicted_summary ) < threshold ):\n",
    "                sentence = np.array([sentence])\n",
    "                #print(sentence, predicted_summary)\n",
    "                predicted_summary.append(sentence)\n",
    "                summary_length += len(nltk.word_tokenize(sentence[0]))\n",
    "                i+=1\n",
    "            model_summary.append(predicted_summary)\n",
    "        except:\n",
    "            print(\" not possible for doc number \"+str(d)+\" because number of sentences in the document greater than 250\")\n",
    "            model_summary.append(\" \")\n",
    "    return model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# text cleaning = stop word removal,special characters removal , POS - Tagging   for inference logic\n",
    "\n",
    "def getWordnetPos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:#We are igonring everything else other than four of the above \n",
    "         # tags. You can add more if you like\n",
    "        return None \n",
    "#Custom function for toeknization\n",
    "def myTokenizer(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas=[]\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        #nltk return the tag from Penntreebank tagsets\n",
    "        sentTag=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        sentTag=[(i.strip(),j) for i,j in sentTag if i.isalpha() and len(i)>2]\n",
    "        #print (sentTag)\n",
    "        for word, tag in sentTag:\n",
    "            # the problem wordnet lemmatizer is that, it recognizes only\n",
    "            # wordnet tags and not the PennTreebank tags. So we shall\n",
    "            # first convert Penntreebank tags to Wordnet tags\n",
    "            wordNetTag=getWordnetPos(tag)\n",
    "            if wordNetTag is None:\n",
    "                continue\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(word,wordNetTag))\n",
    "                \n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "stopWords=nltk.corpus.stopwords.words('english')\n",
    "stopWords+=[\"''\", \"'s\", \"...\", \"``\",\"--\",\"*\",\"-\"]\n",
    "stopWords+=list(string.punctuation)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find similar documents serial numbers which are related to querywords\n",
    "\n",
    "def query_and_op(query_string,number_of_doc, tf_idf):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    \n",
    "    - query_string - input all query words in a single string.\n",
    "    - number_of_doc - number of similar documents to extract\n",
    "    - tf_idf - tf_idf scores saved in a dataframe format where df indexes are vocabulary and columns are docs and cell \n",
    "               values are the tf-idf scores\n",
    "    \n",
    "    output: index of revelent documents or document number best describe the query string words.\n",
    "    \n",
    "    \"\"\"\n",
    "    query = query_string\n",
    "    query_tokens = myTokenizer(query)\n",
    "    \n",
    "    filtered = [] # term-doc\n",
    "    for i in query_tokens:\n",
    "        print(i)\n",
    "        i = i.lower().strip()\n",
    "        try:\n",
    "            i = tf_doc_df.loc[i].values\n",
    "            filtered.append(list(i))\n",
    "\n",
    "        except:\n",
    "            filtered.append([0.0]*tf_doc_df.shape[0])\n",
    "            \n",
    "    doc_term = np.array(filtered).T                   #doc*term\n",
    "    #doc_term.shape                                   #(number of doc * number of term)\n",
    "    tf_idf_score = np.sum(doc_term,axis=1)\n",
    "    most_revelent_doc = np.argsort(-tf_idf_score)[0]  # sort document according to highest similarity score.\n",
    "    \n",
    "    doc_similarity = linear_kernel(doc_term[most_revelent_doc:most_revelent_doc+1],doc_term)\n",
    "    simlar_doc_query_object = np.argsort(-doc_similarity[0])\n",
    "    test_doc = simlar_doc_query_object[:number_of_doc] # top 20 documents describing the query words\n",
    "    \n",
    "    return test_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract summary:\n",
    "\n",
    "def extract_summary(doc_to_query,similarity_indx, title = None,content = None,summary = None,test_rank = None):\n",
    "    test = doc_to_query.loc[test_doc]\n",
    "    test = test.reset_index()\n",
    "    test = test.drop('index',axis = 1)\n",
    "    test = test.reset_index()\n",
    "    test = test.drop('index',axis = 1)\n",
    "    test = test.reset_index()\n",
    "    \n",
    "    test_data = preprocess_data_for_testing(test.index,test.content,test.Summary_regulation)\n",
    "    prediction = fit_predict(test_data)\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        if title:\n",
    "            print(\"Title :\",query_documents.loc[similarity_indx[i]].longtitle,\"\\n\")\n",
    "        if content:\n",
    "            print(\"Content :\",query_documents.loc[similarity_indx[i]].content,\"\\n\")\n",
    "        if summary:\n",
    "            print(\"summary :\",prediction[i][0][0])\n",
    "        if test_rank:\n",
    "            print(\"test_rank :\",query_documents.loc[similarity_indx[i]].Summary_regulation,\"\\n\")\n",
    "            \n",
    "        print(\"\\n**************************\\n\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 248, 1, 50)        45050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 1, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 603       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 45,657\n",
      "Trainable params: 45,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.models.load_model('best_model_cnn.h5')\n",
    "model1.compile(loss='binary_crossentropy',optimizer=Adadelta(),metrics=['mae'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use tf-idf to extract documents and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_documents = pd.read_csv(\"C:/Users/amitp/OneDrive/Ryerson-DS/MRP/regulation.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indx =[]\n",
    "content = []\n",
    "for i,j in enumerate(query_documents.content):\n",
    "    j = nltk.sent_tokenize(j)\n",
    "    if \"Interpretation.\" in j:\n",
    "        ind = j.index(\"Interpretation.\")\n",
    "        indx.append(i)\n",
    "        content.append(j[ind+1:])\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_documents = query_documents.loc[indx]\n",
    "query_documents[\"content\"]= content\n",
    "query_documents = query_documents.reset_index(drop= True)\n",
    "query_documents = query_documents.drop(['consolidationyear','registrationyear','regnalyears', 'shorttitle', 'xrefinternal', 'xrefxternal'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating summary by page- rank as we do not have it for regulation data.\n",
    "# converting string summary into list of sentences\n",
    "Summary_regulation = []\n",
    "\n",
    "for i in query_documents.content:\n",
    "    s  = summarize(\" \".join(i), ratio=0.2)\n",
    "    Summary_regulation.append(s)\n",
    "cont = []    \n",
    "\n",
    "for i in Summary_regulation:\n",
    "    i = nltk.sent_tokenize(i)\n",
    "    cont.append(i)\n",
    "query_documents['Summary_regulation']= cont\n",
    "del Summary_regulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term-document matrix of text corpus: if not already created uncomment this section and genetate these files\n",
    "\n",
    "\n",
    "#documents = pd.read_csv(\"C:/Users/amitp/OneDrive/Ryerson-DS/MRP/regulation.csv\",encoding='utf-8')\n",
    "#documents = documents.loc[indx]\n",
    "#text_corpus = documents.content.values\n",
    "\n",
    "#vectorizer = TfidfVectorizer(max_features=5000, max_df=1.0,tokenizer=myTokenizer, stop_words=stopWords)\n",
    "\n",
    "#tf_doc = vectorizer.fit_transform(text_corpus)\n",
    "#tf_idf_features = vectorizer.get_feature_names()\n",
    "\n",
    "#pickle.dump(tf_doc, open(\"tf_idf_vectorizer.pickle\", \"wb\"))\n",
    "#pickle.dump(tf_idf_features, open(\"tf_idf_features.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### steps to initiate   document searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import if-idf scores . If not already processed then uncomment the above cell get the fils.\n",
    "\n",
    "vectorizer = pickle.load(open(\"tf_idf_vectorizer.pickle\", \"rb\"))\n",
    "features = pickle.load(open(\"tf_idf_features.pickle\", \"rb\"))\n",
    "tf_doc_df = pd.DataFrame(vectorizer.toarray().T,index = features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airport\n",
      "security\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([376,  63, 676, 632], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   give the input words to query stings which will extract the documents numbers.\n",
    "\n",
    "query_string =  \"airport security\"\n",
    "number_of_doc = 4\n",
    "tf_idf = tf_doc_df\n",
    "\n",
    "test_doc = query_and_op( query_string, number_of_doc, tf_doc_df )\n",
    "test_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amitp\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Regulations Respecting Reservations for Airport Apron and Terminal Facilities for Passenger Charter Flights \n",
      "\n",
      "summary : In these Regulations, air carrier  means any person who operates a commercial air service; ( transporteur aérien ) airport manager  means the person in charge of an airport or the authorized representative of that person; ( directeur d’aéroport ) applicant  means an air carrier who applies for a reservation referred to in section 4; ( requérant ) regular charter flight  means a passenger charter flight that is operated by an air carrier on a program basis and for which schedules are provided six months in advance in respect of international flights and three months in advance in respect of domestic flights to the airport manager of each airport listed in the schedule that the air carrier uses for the purposes of the flight, and the apron and terminal facilities of each airport are reserved through established scheduling procedures; ( vol d’affrètement régulier ) reservation  means a reservation of the apron and terminal facilities of an airport listed in the schedule.\n",
      "test_rank : ['In these Regulations, air carrier  means any person who operates a commercial air service; ( transporteur aérien ) airport manager  means the person in charge of an airport or the authorized representative of that person; ( directeur d’aéroport ) applicant  means an air carrier who applies for a reservation referred to in section 4; ( requérant ) regular charter flight  means a passenger charter flight that is operated by an air carrier on a program basis and for which schedules are provided six months in advance in respect of international flights and three months in advance in respect of domestic flights to the airport manager of each airport listed in the schedule that the air carrier uses for the purposes of the flight, and the apron and terminal facilities of each airport are reserved through established scheduling procedures; ( vol d’affrètement régulier ) reservation  means a reservation of the apron and terminal facilities of an airport listed in the schedule.', 'Every air carrier shall, before operating a passenger charter flight, other than a regular charter flight, with an aircraft that has a maximum certified take-off weight of 35,000 pounds or over, apply to the airport manager of each airport listed in the schedule that will be used for the purposes of the flight to reserve the airport apron and terminal facilities needed to accommodate the arrival or departure of the flight.', 'An air carrier shall apply for a reservation by submitting an application therefor to each airport manager at least 30 days before the date of the proposed flight; or where a permit in respect of the proposed flight is issued under the  Air Carrier Regulations  less than 30 days before that date, as soon as possible following the receipt by the air carrier of the permit.', 'Where an air carrier obtains a reservation and subsequently determines that the time specified in the reservation cannot be met, the air carrier shall forthwith notify the airport manager who made the reservation and, where the proposed flight is to be re-scheduled to arrive at or depart from an airport listed in the schedule at another time, the air carrier shall apply for a new reservation.'] \n",
      "\n",
      "\n",
      "**************************\n",
      "\n",
      "Title : Regulations Respecting the Disposal of Personal Property Left at Airports \n",
      "\n",
      "summary : In these Regulations, abandoned vehicle  means a vehicle, other than a derelict vehicle, that has been abandoned at an airport or otherwise remains unclaimed at an airport for a period of not less than 30 days; ( véhicule abandonné ) airport  means an airport or aerodrome under the administration and control of the Minister of Transport; ( aéroport ) Airport Manager  means the Department of Transport official in charge of the airport or his duly authorized representative; ( directeur ) Department  means the Department of Transport; ( ministère ) derelict vehicle  means a vehicle, other than an abandoned vehicle, that has been abandoned at an airport or otherwise remains unclaimed at an airport for a period of not less than 14 days, and has a market value less than $200; ( épave ) owner , with respect to a motor vehicle, means a person who holds legal title to it or a person in whose name it is registered or is required to be registered by the laws of a province and includes a conditional purchaser, lessee or mortgagor who is entitled to or is in possession of the motor vehicle; ( propriétaire ) personal property  means all property, other than a vehicle, not owned by Her Majesty; ( biens personnels ) Regional Administrator  means the Regional Administrator, Canadian Air Transportation Administration, who has jurisdiction over the airport in question; ( administrateur régional ) vehicle  means a self-powered device in, on or by which a person or property is or may be transported or drawn upon a road, except a device used exclusively on stationary rails or tracks.\n",
      "test_rank : ['The Airport Manager shall, within the period referred to in subsection (1), send a notice by registered mail to the owner’s latest known address or place of business advising that, if the vehicle is not claimed within 30 days of the date of such notice, the Regional Administrator may dispose of the vehicle by sale at public auction.', 'The Airport Manager shall, within the period referred to in subsection (1), send a notice by registered mail to the owner’s latest known address or place of business advising that, if the vehicle is not claimed within 20 days of the date of such notice, the Regional Administrator may dispose of the vehicle, at his discretion, by private sale; by sale at public auction; or by destruction.'] \n",
      "\n",
      "\n",
      "**************************\n",
      "\n",
      "Title : Regulations Respecting Zoning at Mayo Airport \n",
      "\n",
      "summary : In these Regulations, airport  means the Mayo Airport, in the vicinity of Mayo, in the Yukon Territory; ( aéroport ) airport reference point  means the point described in Part I of the schedule; ( point de référence de l’aéroport ) approach surfaces  means the imaginary inclined planes that extend upward and outward from each end of a strip, which planes are more particularly described in Part II of the schedule; ( surfaces d’approche ) outer surface  means an imaginary plane located above and in the immediate vicinity of the airport, which plane is more particularly described in Part III of the schedule; ( surface extérieure ) strip  means a rectangular portion of the landing area of the airport, including the runway, that is prepared for the take-off and landing of aircraft in a particular direction, which rectangular portion is more particularly described in Part IV of the schedule; ( bande ) transitional surfaces  means the imaginary inclined planes that extend upward and outward from the lateral limits of a strip and the approach surfaces, which planes are more particularly described in Part V of the schedule.\n",
      "test_rank : ['The outer surface, shown on Mayo Airport Zoning Plan No.', 'E.3028, dated March\\xa028, 1994, is an imaginary plane established at a constant elevation of 45\\xa0m above the elevation of the airport reference point extending to the outer boundary described in Part\\xa0VI, except that, where that plane is less than 9\\xa0m above the surface of the ground, the outer surface is located at 9\\xa0m above the surface of the ground.', 'Each transitional surface, shown on Mayo Airport Zoning Plan No.', 'E.3028, dated March\\xa028, 1994, is an inclined plane rising at a ratio of 1\\xa0m measured vertically to 7\\xa0m measured horizontally at right angles to the centre line and projected centre line of the strip, extending upward and outward from the lateral limits of the strip and its approach surfaces to an intersection with the outer surface.', 'The outer boundary of the land to which these Regulations apply, shown on Mayo Airport Zoning Plan No.', 'E.3028, dated March\\xa028, 1994, is a circle with a radius of 4\\xa0000\\xa0m centred on the airport reference point.'] \n",
      "\n",
      "\n",
      "**************************\n",
      "\n",
      "Title : Regulations Respecting Zoning at St. Anthony Airport \n",
      "\n",
      "summary : In these Regulations, airport  means the St. Anthony Airport, in the Electoral District of Strait of Belle Isle, in the Province of Newfoundland; ( aéroport ) airport reference point  means the point described in Part I of the schedule; ( point de repère de l’aéroport ) approach surfaces  means the imaginary inclined planes that extend upward and outward from each end of a strip, which planes are more particularly described in Part II of the schedule; ( surfaces d’approche ) outer surface  means the imaginary plane located above and in the immediate vicinity of the airport, which plane is more particularly described in Part III of the schedule; ( surface extérieure ) strip  means the rectangular portion of the landing area of the airport, including the runway, prepared for the take-off and landing of aircraft in a particular direction, which portion is more particularly described in Part IV of the schedule; ( bande ) transitional surfaces  means the imaginary inclined planes that extend upward and outward from the lateral limits of a strip and its approach surfaces, which planes are more particularly described in Part V of the schedule.\n",
      "test_rank : ['In these Regulations, airport  means the St. Anthony Airport, in the Electoral District of Strait of Belle Isle, in the Province of Newfoundland; ( aéroport ) airport reference point  means the point described in Part\\xa0I of the schedule; ( point de repère de l’aéroport ) approach surfaces  means the imaginary inclined planes that extend upward and outward from each end of a strip, which planes are more particularly described in Part\\xa0II of the schedule; ( surfaces d’approche ) outer surface  means the imaginary plane located above and in the immediate vicinity of the airport, which plane is more particularly described in Part\\xa0III of the schedule; ( surface extérieure ) strip  means the rectangular portion of the landing area of the airport, including the runway, prepared for the take-off and landing of aircraft in a particular direction, which portion is more particularly described in Part\\xa0IV of the schedule; ( bande ) transitional surfaces  means the imaginary inclined planes that extend upward and outward from the lateral limits of a strip and its approach surfaces, which planes are more particularly described in Part\\xa0V of the schedule.', 'The approach surfaces, shown on St. Anthony Airport Zoning Plans Nos.', 'S-2211-1, S-2211-2 and S-2211-3, dated March\\xa030, 1990, are planes abutting each end of the strip associated with runway\\xa011-29 and are described as follows: an inclined plane abutting the end of the strip associated with the approach to runway\\xa011 having a ratio of 1\\xa0m measured vertically to 50\\xa0m measured horizontally rising to an imaginary horizontal line drawn at right angles to the projected centre line of the strip and distant 15\\xa0000\\xa0m measured horizontally from the end of the strip; the outer ends of the imaginary horizontal line being 2\\xa0400\\xa0m from the projected centre line; said imaginary horizontal line being 300\\xa0m above the elevation at the end of the strip; and an inclined plane abutting the end of the strip associated with the approach to runway\\xa029 having a ratio of 1\\xa0m measured vertically to 50\\xa0m measured horizontally rising to an imaginary horizontal line drawn at right angles to the projected centre line of the strip and distant 15\\xa0000\\xa0m measured horizontally from the end of the strip; the outer ends of the imaginary horizontal line being 2\\xa0400\\xa0m from the projected centre line; said imaginary horizontal line being 300\\xa0m above the elevation at the end of the strip.', 'The outer surface, shown on St. Anthony Airport Zoning Plan No.', 'S-2211-2, dated March\\xa030, 1990, is an imaginary plane established at a constant elevation of 45\\xa0m above the elevation of the airport reference point, except that, where that plane is less than 9\\xa0m above the surface of the ground, the outer surface is located at 9\\xa0m above the surface of the ground.', 'Each transitional surface, shown on St. Anthony Airport Zoning Plan No.', 'S-2211-2, dated March\\xa030, 1990, is an inclined plane rising at a ratio of 1\\xa0m measured vertically to 7\\xa0m measured horizontally at right angles to the centre line and projected centre line of the strip, extending upward and outward from the lateral limits of the strip and its approach surfaces to an intersection with the outer surface.'] \n",
      "\n",
      "\n",
      "**************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the function extract summary as per required setting to get final output\n",
    "\n",
    "doc_to_query = query_documents\n",
    "similarity_indx = test_doc\n",
    "title = True\n",
    "content = False\n",
    "summary = True\n",
    "test_rank = True\n",
    "\n",
    "extract_summary(doc_to_query,similarity_indx, title,content,summary,test_rank)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
